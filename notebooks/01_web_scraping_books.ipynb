{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdf08adb-7d77-445c-8ffb-fad7805cd634",
   "metadata": {},
   "source": [
    "# ============== WEB SCRAPING BOOKS.TOSCRAPE.COM ================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4719aeeb-1340-4d13-9b54-6cada1e6e990",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3164cd77-9d73-443c-b788-02dd2932ea92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1/50...\n",
      "Scraping page 2/50...\n",
      "Scraping page 3/50...\n",
      "Scraping page 4/50...\n",
      "Scraping page 5/50...\n",
      "Scraping page 6/50...\n",
      "Scraping page 7/50...\n",
      "Scraping page 8/50...\n",
      "Scraping page 9/50...\n",
      "Scraping page 10/50...\n",
      "Scraping page 11/50...\n",
      "Scraping page 12/50...\n",
      "Scraping page 13/50...\n",
      "Scraping page 14/50...\n",
      "Scraping page 15/50...\n",
      "Scraping page 16/50...\n",
      "Scraping page 17/50...\n",
      "Scraping page 18/50...\n",
      "Scraping page 19/50...\n",
      "Scraping page 20/50...\n",
      "Scraping page 21/50...\n",
      "Scraping page 22/50...\n",
      "Scraping page 23/50...\n",
      "Scraping page 24/50...\n",
      "Scraping page 25/50...\n",
      "Scraping page 26/50...\n",
      "Scraping page 27/50...\n",
      "Scraping page 28/50...\n",
      "Scraping page 29/50...\n",
      "Scraping page 30/50...\n",
      "Scraping page 31/50...\n",
      "Scraping page 32/50...\n",
      "Scraping page 33/50...\n",
      "Scraping page 34/50...\n",
      "Scraping page 35/50...\n",
      "Scraping page 36/50...\n",
      "Scraping page 37/50...\n",
      "Scraping page 38/50...\n",
      "Scraping page 39/50...\n",
      "Scraping page 40/50...\n",
      "Scraping page 41/50...\n",
      "Scraping page 42/50...\n",
      "Scraping page 43/50...\n",
      "Scraping page 44/50...\n",
      "Scraping page 45/50...\n",
      "Scraping page 46/50...\n",
      "Scraping page 47/50...\n",
      "Scraping page 48/50...\n",
      "Scraping page 49/50...\n",
      "Scraping page 50/50...\n",
      "Scraping terminé ! 1000 livres collectés.\n",
      "Fichiers sauvegardés : books_toscrape_complete.csv et .json\n"
     ]
    }
   ],
   "source": [
    "# Liste pour stocker tous les livres\n",
    "all_books = []\n",
    "\n",
    "# URL de base\n",
    "base_url = \"http://books.toscrape.com/catalogue/page-{}.html\"\n",
    "# Il y a 50 pages\n",
    "for page in range(1, 51):\n",
    "    print(f\"Scraping page {page}/50...\")\n",
    "    url = base_url.format(page)\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(\"Erreur de connexion à la page\", page)\n",
    "        continue\n",
    "        \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Tous les livres de la page\n",
    "    books = soup.find_all('article', class_='product_pod')\n",
    "    \n",
    "    for book in books:\n",
    "        title = book.h3.a['title']\n",
    "        price = book.find('p', class_='price_color').text[1:]  \n",
    "        price = float(price)\n",
    "        \n",
    "        # Rating (One, Two, Three, Four, Five)\n",
    "        rating_class = book.find('p', class_='star-rating')['class'][1]\n",
    "        rating_map = {'One':1, 'Two':2, 'Three':3, 'Four':4, 'Five':5}\n",
    "        rating = rating_map.get(rating_class, 0)\n",
    "        \n",
    "        # Disponibilité\n",
    "        stock = book.find('p', class_='instock availability').text.strip()\n",
    "        in_stock = \"In stock\" in stock\n",
    "        \n",
    "        # Lien détaillé\n",
    "        book_url = \"http://books.toscrape.com/catalogue/\" + book.h3.a['href'].replace('../../../', '')\n",
    "        \n",
    "        # Catégorie (on va chercher dans la page détaillée)\n",
    "        detail_response = requests.get(book_url)\n",
    "        detail_soup = BeautifulSoup(detail_response.content, 'html.parser')\n",
    "        category = detail_soup.find('ul', class_='breadcrumb').find_all('li')[2].text.strip()\n",
    "        \n",
    "        all_books.append({\n",
    "            'Title': title,\n",
    "            'Price (£)': price,\n",
    "            'Rating (1-5)': rating,\n",
    "            'In Stock': in_stock,\n",
    "            'Category': category,\n",
    "            'URL': book_url\n",
    "        })\n",
    "    \n",
    "    # Politeness : petite pause pour ne pas surcharger le serveur\n",
    "    time.sleep(random.uniform(1, 2))\n",
    "\n",
    "# Conversion en DataFrame\n",
    "df = pd.DataFrame(all_books)\n",
    "print(f\"Scraping terminé ! {len(df)} livres collectés.\")\n",
    "\n",
    "# Sauvegarde\n",
    "df.to_csv('../data/books_toscrape_complete.csv', index=False)\n",
    "df.to_json('../data/books_toscrape_complete.json', orient='records', indent=4)\n",
    "\n",
    "print(\"Fichiers sauvegardés : books_toscrape_complete.csv et .json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
